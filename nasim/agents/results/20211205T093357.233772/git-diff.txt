diff --git a/nasim/agents/a3c_agent.py b/nasim/agents/a3c_agent.py
index dead89d..ed43470 100644
--- a/nasim/agents/a3c_agent.py
+++ b/nasim/agents/a3c_agent.py
@@ -1,98 +1,213 @@
-"""An example A3C Agent.
+"""An example of training A3C against OpenAI Gym Envs.
+This script is an example of training a A3C agent against OpenAI Gym envs.
+Both discrete and continuous action spaces are supported.
+To solve CartPole-v0, run:
+    python train_a3c_gym.py 8 --env CartPole-v0
+To solve InvertedPendulum-v1, run:
+    python train_a3c_gym.py 8 --env InvertedPendulum-v1 --arch LSTMGaussian --t-max 50  # noqa
+"""
+import argparse
+import os
 
-It uses pytorch 1.5+ tensorboard library for logging (HINT: these dependencies
-can be installed by running pip install nasim[dqn])
+# This prevents numpy from using multiple threads
+os.environ['OMP_NUM_THREADS'] = '1'  # NOQA
 
-To run 'tiny' benchmark scenario with default settings, run the following from
-the nasim/agents dir:
+import chainer
+from chainer import functions as F
+from chainer import links as L
+import nasim
+import numpy as np
 
-$ python a3c_agent.py tiny
+import chainerrl
+from chainerrl.agents import a3c
+from chainerrl import experiments
+from chainerrl import links
+from chainerrl import misc
+from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay
+from chainerrl.optimizers import rmsprop_async
+from chainerrl import policies
+from chainerrl.recurrent import RecurrentChainMixin
+from chainerrl import v_function
 
-To see detailed results using tensorboard:
+class A3CFFSoftmax(chainer.ChainList, a3c.A3CModel):
+    """An example of A3C feedforward softmax policy."""
 
-$ tensorboard --logdir runs/
+    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):
+        self.pi = policies.SoftmaxPolicy(
+            model=links.MLP(ndim_obs, n_actions, hidden_sizes))
+        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)
+        super().__init__(self.pi, self.v)
 
-To see available hyperparameters:
+    def pi_and_v(self, state):
+        return self.pi(state), self.v(state)
 
-$ python a3c_agent.py --help
 
-Notes
------
+class A3CFFMellowmax(chainer.ChainList, a3c.A3CModel):
+    """An example of A3C feedforward mellowmax policy."""
 
+    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):
+        self.pi = policies.MellowmaxPolicy(
+            model=links.MLP(ndim_obs, n_actions, hidden_sizes))
+        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)
+        super().__init__(self.pi, self.v)
 
-"""
+    def pi_and_v(self, state):
+        return self.pi(state), self.v(state)
 
-import random
-import numpy as np
-from gym import error
-from pprint import pprint
 
-import nasim
+class A3CLSTMGaussian(chainer.ChainList, a3c.A3CModel, RecurrentChainMixin):
+    """An example of A3C recurrent Gaussian policy."""
+
+    def __init__(self, obs_size, action_size, hidden_size=200, lstm_size=128):
+        self.pi_head = L.Linear(obs_size, hidden_size)
+        self.v_head = L.Linear(obs_size, hidden_size)
+        self.pi_lstm = L.LSTM(hidden_size, lstm_size)
+        self.v_lstm = L.LSTM(hidden_size, lstm_size)
+        self.pi = policies.FCGaussianPolicy(lstm_size, action_size)
+        self.v = v_function.FCVFunction(lstm_size)
+        super().__init__(self.pi_head, self.v_head,
+                         self.pi_lstm, self.v_lstm, self.pi, self.v)
+
+    def pi_and_v(self, state):
+
+        def forward(head, lstm, tail):
+            h = F.relu(head(state))
+            h = lstm(h)
+            return tail(h)
+
+        pout = forward(self.pi_head, self.pi_lstm, self.pi)
+        vout = forward(self.v_head, self.v_lstm, self.v)
+
+        return pout, vout
+
+
+def main():
+    import logging
 
-try:
-    from torch.utils.tensorboard import SummaryWriter
-except ImportError as e:
-    raise error.DependencyNotInstalled(
-        f"{e}. (HINT: you can install a3c_agent dependencies by running "
-        "'pip install nasim[dqn]'.)"
-    )
-
-class Memory():
-    def __init__(self):
-        self.states = []
-        self.actions = []
-        self.rewards = []
-        
-    def store(self, state, action, reward):
-        self.states.append(state)
-        self.actions.append(action)
-        self.rewards.append(reward)
-        
-    def clear(self):
-        self.states = []
-        self.actions = []
-        self.rewards = []
-
-class A3C():
-
-class A3CAgent():
-
-if __name__ == "__main__":
-    import argparse
     parser = argparse.ArgumentParser()
-    parser.add_argument("env_name", type=str, help="benchmark scenario name")
-    parser.add_argument("--render_eval", action="store_true",
-                        help="Renders final policy")
-    parser.add_argument("--lr", type=float, default=0.001,
-                        help="Learning rate (default=0.001)")
-    parser.add_argument("-t", "--training_steps", type=int, default=10000,
-                        help="training steps (default=10000)")
-    parser.add_argument("--batch_size", type=int, default=32,
-                        help="(default=32)")
-    parser.add_argument("--seed", type=int, default=0,
-                        help="(default=0)")
-    parser.add_argument("--replay_size", type=int, default=100000,
-                        help="(default=100000)")
-    parser.add_argument("--final_epsilon", type=float, default=0.05,
-                        help="(default=0.05)")
-    parser.add_argument("--init_epsilon", type=float, default=1.0,
-                        help="(default=1.0)")
-    parser.add_argument("--exploration_steps", type=int, default=10000,
-                        help="(default=10000)")
-    parser.add_argument("--gamma", type=float, default=0.99,
-                        help="(default=0.99)")
-    parser.add_argument("--quite", action="store_false",
-                        help="Run in Quite mode")
+    parser.add_argument('processes', type=int)
+    parser.add_argument('--env', type=str, default='CartPole-v0')
+    parser.add_argument('--arch', type=str, default='FFSoftmax',
+                        choices=('FFSoftmax', 'FFMellowmax', 'LSTMGaussian'))
+    parser.add_argument('--seed', type=int, default=0,
+                        help='Random seed [0, 2 ** 32)')
+    parser.add_argument('--outdir', type=str, default='results',
+                        help='Directory path to save output files.'
+                            ' If it does not exist, it will be created.')
+    parser.add_argument('--t-max', type=int, default=5)
+    parser.add_argument('--beta', type=float, default=1e-2)
+    parser.add_argument('--profile', action='store_true')
+    parser.add_argument('--steps', type=int, default=8 * 10 ** 7)
+    parser.add_argument('--eval-interval', type=int, default=10 ** 5)
+    parser.add_argument('--eval-n-runs', type=int, default=10)
+    parser.add_argument('--reward-scale-factor', type=float, default=1e-2)
+    parser.add_argument('--rmsprop-epsilon', type=float, default=1e-1)
+    parser.add_argument('--render', action='store_true', default=False)
+    parser.add_argument('--lr', type=float, default=7e-4)
+    parser.add_argument('--weight-decay', type=float, default=0.0)
+    parser.add_argument('--demo', action='store_true', default=False)
+    parser.add_argument('--load', type=str, default='')
+    parser.add_argument('--logger-level', type=int, default=logging.DEBUG)
+    parser.add_argument('--monitor', action='store_true')
+    parser.add_argument('--num-env', type=int, default=1)
     args = parser.parse_args()
 
-    env = nasim.make_benchmark(
-        args.env_name,
-        args.seed,
-        fully_obs=True,
-        flat_actions=True,
-        flat_obs=True
-    )
-
-    a3c_agent = A3CAgent(env, verbose=args.quite, **vars(args))
-    a3c_agent.train()
-    a3c_agent.run_eval_episode(render=args.render_eval)
\ No newline at end of file
+    logging.basicConfig(level=args.logger_level)
+
+    # Set a random seed used in ChainerRL.
+    # If you use more than one processes, the results will be no longer
+    # deterministic even with the same random seed.
+    misc.set_random_seed(args.seed)
+
+    # Set different random seeds for different subprocesses.
+    # If seed=0 and processes=4, subprocess seeds are [0, 1, 2, 3].
+    # If seed=1 and processes=4, subprocess seeds are [4, 5, 6, 7].
+    process_seeds = np.arange(args.processes) + args.seed * args.processes
+    assert process_seeds.max() < 2 ** 32
+
+    args.outdir = experiments.prepare_output_dir(args, args.outdir)
+
+    def make_env(process_idx, test):
+        env = nasim.load("../scenarios/own_S/Permutations/Own_S_"+ str(process_idx%args.num_env) +".yaml", flat_actions=True, flat_obs=True)
+        # Use different random seeds for train and test envs
+        #process_seed = int(process_seeds[process_idx])
+        #env_seed = 2 ** 32 - 1 - process_seed if test else process_seed
+        #env.seed(env_seed)
+        # Cast observations to float32 because our model uses float32
+        env = chainerrl.wrappers.CastObservationToFloat32(env)
+        if args.monitor and process_idx == 0:
+            env = chainerrl.wrappers.Monitor(env, args.outdir)
+        if not test:
+            # Scale rewards (and thus returns) to a reasonable range so that
+            # training is easier
+            env = chainerrl.wrappers.ScaleReward(env, args.reward_scale_factor)
+        if args.render and process_idx == 0 and not test:
+            env = chainerrl.wrappers.Render(env)
+        return env
+
+    sample_env = nasim.load("../scenarios/own_S/Permutations/Own_S_0.yaml", flat_actions=True, flat_obs=True)
+    #timestep_limit = sample_env.spec.max_episode_steps
+    timestep_limit = 500
+    obs_space = sample_env.observation_space
+    action_space = sample_env.action_space
+
+    # Switch policy types accordingly to action space types
+    if args.arch == 'LSTMGaussian':
+        model = A3CLSTMGaussian(obs_space.low.size, 1)
+    elif args.arch == 'FFSoftmax':
+        model = A3CFFSoftmax(obs_space.low.size, action_space.n)
+    elif args.arch == 'FFMellowmax':
+        model = A3CFFMellowmax(obs_space.low.size, action_space.n)
+
+    opt = rmsprop_async.RMSpropAsync(
+        lr=args.lr, eps=args.rmsprop_epsilon, alpha=0.99)
+    opt.setup(model)
+    opt.add_hook(chainer.optimizer.GradientClipping(40))
+    if args.weight_decay > 0:
+        opt.add_hook(NonbiasWeightDecay(args.weight_decay))
+
+    agent = a3c.A3C(model, opt, t_max=args.t_max, gamma=0.99,
+                    beta=args.beta, act_deterministically=False)
+    if args.load:
+        agent.load(args.load)
+
+    if args.demo:
+        env = make_env(0, True)
+        eval_stats = experiments.eval_performance(
+            env=env,
+            agent=agent,
+            n_steps=None,
+            n_episodes=args.eval_n_runs,
+            max_episode_len=timestep_limit)
+        print('n_runs: {} mean: {} median: {} stdev {}'.format(
+            args.eval_n_runs, eval_stats['mean'], eval_stats['median'],
+            eval_stats['stdev']))
+
+        env = make_env(1, True)
+        eval_stats = experiments.eval_performance(
+            env=env,
+            agent=agent,
+            n_steps=None,
+            n_episodes=args.eval_n_runs,
+            max_episode_len=timestep_limit)
+        print('n_runs: {} mean: {} median: {} stdev {}'.format(
+            args.eval_n_runs, eval_stats['mean'], eval_stats['median'],
+            eval_stats['stdev']))
+
+    else:
+        experiments.train_agent_async(
+            agent=agent,
+            outdir=args.outdir,
+            processes=args.processes,
+            make_env=make_env,
+            profile=args.profile,
+            steps=args.steps,
+            eval_n_steps=None,
+            eval_n_episodes=args.eval_n_runs,
+            eval_interval=args.eval_interval,
+            max_episode_len=timestep_limit)
+
+    
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
